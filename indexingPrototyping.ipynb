{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "# prototyping the indexing\n",
    "## getting all the file paths\n",
    "\n",
    "> TODO use generators / yield to prevent loading all into RAM? maybe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------ Adhoc\n",
      "--------- fr94\n",
      "------------ 06\n",
      "------------ 09\n",
      "------------ 03\n",
      "------------ 11\n",
      "------------ 01\n",
      "------------ 10\n",
      "------------ 04\n",
      "------------ 12\n",
      "------------ 08\n",
      "------------ 07\n",
      "------------ 05\n",
      "------------ 02\n",
      "--------- ft\n",
      "------------ ft941\n",
      "------------ ft934\n",
      "------------ ft931\n",
      "------------ ft944\n",
      "------------ ft924\n",
      "------------ ft923\n",
      "------------ ft943\n",
      "------------ ft933\n",
      "------------ ft942\n",
      "------------ ft932\n",
      "------------ ft921\n",
      "------------ ft911\n",
      "------------ ft922\n",
      "--------- fbis\n",
      "--------- latimes\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "sys.version_info(major=3, minor=6, micro=0, releaselevel='final', serial=0)"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# traverse root directory, and list directories as dirs and files as files\n",
    "for root, dirs, files in os.walk(\"data/TREC8all/Adhoc\"):\n",
    "    path = root.split(os.sep)\n",
    "    print((len(path) - 1) * '---', os.path.basename(root))\n",
    "    # commented out to keep output short\n",
    "    #for file in files:\n",
    "    #    print(len(path) * '---', file)\n",
    "import sys\n",
    "sys.version\n",
    "sys.version_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "documents_path = 'data/TREC8all/Adhoc'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fs.exists(documents_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['indexer.sh',\n",
       " 'indexingPrototyping.ipynb',\n",
       " 'Untitled Document',\n",
       " 'searcher.sh',\n",
       " 'LICSENSE',\n",
       " 'travis.yml',\n",
       " 'README.md',\n",
       " 'assignment.pdf']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(fs.list())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/jakob/advancedInformationRetrieval/project1/data/TREC8all/Adhoc/fr94/06/fr9406210'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(fs.find(pattern='*[!.]', path=documents_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "\n",
    "for filenames in fs.find(documents_path):\n",
    "    print(filenames)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## first read an sgml file and parse the tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false,
    "deletable": true,
    "editable": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<docno> FR940302-0-00001 </docno>,\n",
       " <docno> FR940302-0-00002 </docno>,\n",
       " <docno> FR940302-0-00003 </docno>,\n",
       " <docno> FR940302-0-00004 </docno>,\n",
       " <docno> FR940302-0-00005 </docno>,\n",
       " <docno> FR940302-0-00006 </docno>,\n",
       " <docno> FR940302-0-00007 </docno>,\n",
       " <docno> FR940302-0-00008 </docno>,\n",
       " <docno> FR940302-0-00009 </docno>,\n",
       " <docno> FR940302-0-00010 </docno>,\n",
       " <docno> FR940302-0-00011 </docno>,\n",
       " <docno> FR940302-0-00012 </docno>,\n",
       " <docno> FR940302-0-00013 </docno>,\n",
       " <docno> FR940302-0-00014 </docno>,\n",
       " <docno> FR940302-0-00015 </docno>,\n",
       " <docno> FR940302-0-00016 </docno>,\n",
       " <docno> FR940302-0-00017 </docno>,\n",
       " <docno> FR940302-0-00018 </docno>,\n",
       " <docno> FR940302-0-00019 </docno>,\n",
       " <docno> FR940302-0-00020 </docno>,\n",
       " <docno> FR940302-0-00021 </docno>,\n",
       " <docno> FR940302-0-00022 </docno>,\n",
       " <docno> FR940302-0-00023 </docno>,\n",
       " <docno> FR940302-0-00024 </docno>,\n",
       " <docno> FR940302-0-00025 </docno>,\n",
       " <docno> FR940302-0-00026 </docno>,\n",
       " <docno> FR940302-0-00027 </docno>,\n",
       " <docno> FR940302-0-00028 </docno>,\n",
       " <docno> FR940302-0-00029 </docno>,\n",
       " <docno> FR940302-0-00030 </docno>,\n",
       " <docno> FR940302-0-00031 </docno>,\n",
       " <docno> FR940302-0-00032 </docno>,\n",
       " <docno> FR940302-0-00033 </docno>,\n",
       " <docno> FR940302-0-00034 </docno>]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "path = 'data/TREC8all/Adhoc/fr94/03/fr9403020'\n",
    "text_file = open(path, 'r').read()\n",
    "soup = BeautifulSoup(text_file,'html.parser')\n",
    "soup.find_all('docno')[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## tokenization sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import en_core_web_md\n",
    "import spacy\n",
    "from spacy.symbols import ORTH, LEMMA, POS\n",
    "\n",
    "nlp = en_core_web_md.load()\n",
    "doc = nlp(u'This is a sentence.')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "contents of default pipeline. To customize use https://spacy.io/docs/usage/customizing-pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<spacy.tagger.Tagger object at 0x7fb1727a10d8>\n",
      "<spacy.pipeline.DependencyParser object at 0x7fb172086ef8>\n",
      "<spacy.matcher.Matcher object at 0x7fb12f4f3518>\n",
      "<spacy.pipeline.EntityRecognizer object at 0x7fb1332c93b8>\n"
     ]
    }
   ],
   "source": [
    "for proc in nlp.pipeline:\n",
    "    print(proc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "lemmatization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: this, lemma2: this\n",
      "530\n",
      "token: is, lemma2: be\n",
      "522\n",
      "token: spacy, lemma2: spacy\n",
      "173815\n",
      "token: lemmatize, lemma2: lemmatize\n",
      "1484778\n",
      "token: testing, lemma2: testing\n",
      "2933\n",
      "token: ., lemma2: .\n",
      "453\n",
      "token: programming, lemma2: programming\n",
      "3441\n",
      "token: books, lemma2: book\n",
      "1045\n",
      "token: are, lemma2: be\n",
      "522\n",
      "token: more, lemma2: more\n",
      "563\n",
      "token: better, lemma2: better\n",
      "649\n",
      "token: than, lemma2: than\n",
      "589\n",
      "token: others, lemma2: other\n",
      "598\n",
      "################\n",
      "this is spacy lemmatize testing.\n",
      "programming books are more better than others\n",
      "################\n",
      "original: 530 this\n",
      "lowercased: 530 this\n",
      "lemma: 530 this\n",
      "shape: 53773 xxxx\n",
      "prefix: 3631 t\n",
      "suffix: 592 his\n",
      "log probability: -5.36181640625\n",
      "Brown cluster id: 63\n",
      "----------------------------------------\n",
      "original: 508 is\n",
      "lowercased: 508 is\n",
      "lemma: 522 be\n",
      "shape: 21614 xx\n",
      "prefix: 604 i\n",
      "suffix: 508 is\n",
      "log probability: -4.457748889923096\n",
      "Brown cluster id: 762\n",
      "----------------------------------------\n",
      "original: 173815 spacy\n",
      "lowercased: 173815 spacy\n",
      "lemma: 173815 spacy\n",
      "shape: 53773 xxxx\n",
      "prefix: 1046 s\n",
      "suffix: 412245 acy\n",
      "log probability: -17.099441528320312\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 1484778 lemmatize\n",
      "lowercased: 1484778 lemmatize\n",
      "lemma: 1484778 lemmatize\n",
      "shape: 53773 xxxx\n",
      "prefix: 10476 l\n",
      "suffix: 131527 ize\n",
      "log probability: -19.502029418945312\n",
      "Brown cluster id: 0\n",
      "----------------------------------------\n",
      "original: 2933 testing\n",
      "lowercased: 2933 testing\n",
      "lemma: 2933 testing\n",
      "shape: 53773 xxxx\n",
      "prefix: 3631 t\n",
      "suffix: 12884 ing\n",
      "log probability: -10.473899841308594\n",
      "Brown cluster id: 3050\n",
      "----------------------------------------\n",
      "original: 453 .\n",
      "lowercased: 453 .\n",
      "lemma: 453 .\n",
      "shape: 453 .\n",
      "prefix: 453 .\n",
      "suffix: 453 .\n",
      "log probability: -3.0678977966308594\n",
      "Brown cluster id: 8\n",
      "----------------------------------------\n",
      "original: 3441 programming\n",
      "lowercased: 3441 programming\n",
      "lemma: 3441 programming\n",
      "shape: 53773 xxxx\n",
      "prefix: 10182 p\n",
      "suffix: 12884 ing\n",
      "log probability: -10.708765029907227\n",
      "Brown cluster id: 15993\n",
      "----------------------------------------\n",
      "original: 1373 books\n",
      "lowercased: 1373 books\n",
      "lemma: 1045 book\n",
      "shape: 53773 xxxx\n",
      "prefix: 3546 b\n",
      "suffix: 322019 oks\n",
      "log probability: -9.2781400680542\n",
      "Brown cluster id: 1677\n",
      "----------------------------------------\n",
      "original: 526 are\n",
      "lowercased: 526 are\n",
      "lemma: 522 be\n",
      "shape: 29016 xxx\n",
      "prefix: 503 a\n",
      "suffix: 526 are\n",
      "log probability: -5.271068096160889\n",
      "Brown cluster id: 1530\n",
      "----------------------------------------\n",
      "original: 563 more\n",
      "lowercased: 563 more\n",
      "lemma: 563 more\n",
      "shape: 53773 xxxx\n",
      "prefix: 1011 m\n",
      "suffix: 13711 ore\n",
      "log probability: -6.081598281860352\n",
      "Brown cluster id: 1514\n",
      "----------------------------------------\n",
      "original: 649 better\n",
      "lowercased: 649 better\n",
      "lemma: 649 better\n",
      "shape: 53773 xxxx\n",
      "prefix: 3546 b\n",
      "suffix: 28383 ter\n",
      "log probability: -7.1072587966918945\n",
      "Brown cluster id: 7658\n",
      "----------------------------------------\n",
      "original: 589 than\n",
      "lowercased: 589 than\n",
      "lemma: 589 than\n",
      "shape: 53773 xxxx\n",
      "prefix: 3631 t\n",
      "suffix: 11118 han\n",
      "log probability: -6.512253761291504\n",
      "Brown cluster id: 106\n",
      "----------------------------------------\n",
      "original: 905 others\n",
      "lowercased: 905 others\n",
      "lemma: 598 other\n",
      "shape: 53773 xxxx\n",
      "prefix: 2525 o\n",
      "suffix: 43026 ers\n",
      "log probability: -8.396527290344238\n",
      "Brown cluster id: 1901\n",
      "----------------------------------------\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"this is spacy lemmatize testing. programming books are more better than others\")\n",
    "\n",
    "for token in doc3:\n",
    "    print('token: ' + str(token) + \", lemma2: \" + str(token.lemma_))\n",
    "    print(token.lemma)\n",
    "# print sentences\n",
    "print(\"################\")\n",
    "for sent in doc3.sents:\n",
    "    print(sent)\n",
    "\n",
    "print(\"################\")\n",
    "    \n",
    "for i, token in enumerate(doc3):\n",
    "    print(\"original:\", token.orth, token.orth_)\n",
    "    print(\"lowercased:\", token.lower, token.lower_)\n",
    "    print(\"lemma:\", token.lemma, token.lemma_)\n",
    "    print(\"shape:\", token.shape, token.shape_)\n",
    "    print(\"prefix:\", token.prefix, token.prefix_)\n",
    "    print(\"suffix:\", token.suffix, token.suffix_)\n",
    "    print(\"log probability:\", token.prob)\n",
    "    print(\"Brown cluster id:\", token.cluster)\n",
    "    print(\"----------------------------------------\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "handling of file path i.e. not loosing the connection / information to the original file\n",
    "http://stackoverflow.com/questions/42979472/inverted-index-in-python-with-spacy-as-tokenization-and-persistent-relation-to-o\n",
    "\n",
    "but how to fill it?\n",
    "\n",
    "some info is required to be stored: https://tuwel.tuwien.ac.at/mod/forum/discuss.php?d=85807"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc3.user_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "THIS\n",
      "IS\n",
      "A\n",
      "SENTENCE\n",
      ".\n"
     ]
    }
   ],
   "source": [
    "for w in nlp(u'This is a sentence.'.upper()):\n",
    "    print(w.text)\n",
    "#    print(w.pos)\n",
    "#    print(w.text_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "multi threaded tokenization using generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "texts = [u'One document.', u'...', u'Lots of documents']\n",
    "# .pipe streams input, and produces streaming output\n",
    "iter_texts = (texts[i % 3] for i in range(100000000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n",
      "One document.\n",
      "...\n",
      "Lots of documents\n"
     ]
    }
   ],
   "source": [
    "# todo pick a way bigger batch size\n",
    "for i, doc in enumerate(nlp.pipe(iter_texts, batch_size=50, n_threads=4)):\n",
    "    assert doc.is_parsed\n",
    "    if i == 30:\n",
    "        break\n",
    "    #print(i)\n",
    "    print(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['gimme', 'that']"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "[w.text for w in nlp(u'gimme that')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stemming\n",
    "\n",
    "- https://github.com/explosion/spaCy/issues/327\n",
    "\n",
    "Demonstrates basic stemming below. Now we just need to make sure that the tokens we obtained beforehand will play nicely"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import snowballstemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['endosom', 'foo', 'bar', 'tomoorow', 'I', 'go', 'to', 'univers']"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stemmer = snowballstemmer.stemmer(\"english\")\n",
    "stemmer.stemWords(\"endosomes foo bar tomoorow I go to university\".split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "token: this, lemma2: this stemmer: this\n",
      "token: is, lemma2: be stemmer: is\n",
      "token: spacy, lemma2: spacy stemmer: spaci\n",
      "token: lemmatize, lemma2: lemmatize stemmer: lemmat\n",
      "token: testing, lemma2: testing stemmer: test\n",
      "token: ., lemma2: . stemmer: .\n",
      "token: programming, lemma2: programming stemmer: program\n",
      "token: books, lemma2: book stemmer: book\n",
      "token: are, lemma2: be stemmer: are\n",
      "token: more, lemma2: more stemmer: more\n",
      "token: better, lemma2: better stemmer: better\n",
      "token: than, lemma2: than stemmer: than\n",
      "token: others, lemma2: other stemmer: other\n"
     ]
    }
   ],
   "source": [
    "doc3 = nlp(u\"this is spacy lemmatize testing. programming books are more better than others\")\n",
    "\n",
    "for token in doc3:\n",
    "    print('token: ' + str(token)+ \", lemma2: \" + str(token.lemma_) +\n",
    "          \" stemmer: \" + stemmer.stemWords(token.text.split())[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## stopwords\n",
    "called special words?\n",
    "\n",
    "check out how to add a custom stopword. https://github.com/explosion/spaCy/issues/226"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'error' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-29dc82fb8455>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0merror\u001b[0m\u001b[0;31m#. do not yet run below code.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'error' is not defined"
     ]
    }
   ],
   "source": [
    "error#. do not yet run below code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": true,
    "editable": true
   },
   "source": [
    "## indexing sample\n",
    "- first step is to perform tokenization (with options from the commandline, + normalization)\n",
    "- build the index\n",
    "\n",
    "What to store:\n",
    "- each document has an id number: `<num> Number: 401` which is translated into and id by `qrels.trec8.adhoc.parts1-5`\n",
    "\n",
    "For the persistence of the index pickle could be used \n",
    "\n",
    "https://nlp.stanford.edu/IR-book/html/htmledition/single-pass-in-memory-indexing-1.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#input = [word1, word2, ...]\n",
    "#output = {word1: [pos1, pos2], word2: [pos2, pos434], ...}\n",
    "def index_one_file(termlist):\n",
    "\tfileIndex = {}\n",
    "\tfor index, word in enumerate(termlist):\n",
    "\t\tif word in fileIndex.keys():\n",
    "\t\t\tfileIndex[word].append(index)\n",
    "\t\telse:\n",
    "\t\t\tfileIndex[word] = [index]\n",
    "\treturn fileIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#input = {filename: [word1, word2, ...], ...}\n",
    "#res = {filename: {word: [pos1, pos2, ...]}, ...}\n",
    "def make_indices(termlists):\n",
    "\ttotal = {}\n",
    "\tfor filename in termlists.keys():\n",
    "\t\ttotal[filename] = index_one_file(termlists[filename])\n",
    "\treturn total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dict reversal http://stackoverflow.com/questions/35945473/how-to-reverse-a-dictionary-in-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true,
    "deletable": true,
    "editable": true
   },
   "outputs": [],
   "source": [
    "#input = {filename: {word: [pos1, pos2, ...], ... }}\n",
    "#res = {word: {filename: [pos1, pos2]}, ...}, ...}\n",
    "def fullIndex(regdex):\n",
    "\ttotal_index = {}\n",
    "\tfor filename in regdex.keys():\n",
    "\t\tfor word in regdex[filename].keys():\n",
    "\t\t\tif word in total_index.keys():\n",
    "\t\t\t\tif filename in total_index[word].keys():\n",
    "\t\t\t\t\ttotal_index[word][filename].extend(regdex[filename][word][:])\n",
    "\t\t\t\telse:\n",
    "\t\t\t\t\ttotal_index[word][filename] = regdex[filename][word]\n",
    "\t\t\telse:\n",
    "\t\t\t\ttotal_index[word] = {filename: regdex[filename][word]}\n",
    "\treturn total_index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "second idea to kickstart inverted indexing http://stackoverflow.com/questions/28019543/inverted-index-given-a-list-of-document-tokens-using-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "defaultdict(<class 'list'>, {'a': [0, 1], 'b': [0], 'c': [1]})\n",
      "dict_keys(['a', 'b', 'c'])\n",
      "[0, 1]\n",
      "[0]\n"
     ]
    }
   ],
   "source": [
    "from collections import defaultdict\n",
    "def create_index (data):\n",
    "    index = defaultdict(list)\n",
    "    for i, tokens in enumerate(data):\n",
    "        for token in tokens:\n",
    "            index[token].append(i)\n",
    "    return index\n",
    "\n",
    "index = create_index([['a', 'b'], ['a', 'c']])\n",
    "print(index)\n",
    "print(index.keys())\n",
    "print(index['a'])\n",
    "print(index['b'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## punctuation\n",
    "- addd additional punctiation characters https://nicschrading.com/project/Intro-to-NLP-with-spaCy/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
